{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "de-en training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLgyB4DaIZAkgRmz0TB/BO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Isometric-Multilingual-Speech-Translation-using-Multi-Task-Learning-Framework/blob/main/de_en_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yPpIQCUTu9pf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# un-comment below, while working on colab.\n",
        "!pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece] wandb boto3 --quiet "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import boto3\n",
        "import os\n",
        "import random\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm \n",
        "import wandb\n",
        "import logging\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1sNQTEXAwBB-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for logging loss to wandb.ai\n",
        "access_key = \"c7deb1bb77ce9433eb246d460385f363659145a8\" # enter wandb secret_accces_key\n",
        "wandb.login(key=access_key)"
      ],
      "metadata": {
        "id": "3f0qpSQnwM1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "from datasets import load_dataset, load_metric\n",
        "raw_datasets = load_dataset(\"enimai/MuST-C-de\")\n",
        "metric = load_metric(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "SS5bSKnSwOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset description\n",
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "luBld-SZwSPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model checkpoints\n",
        "train_model_checkpoints = \"Helsinki-NLP/opus-mt-de-fr\""
      ],
      "metadata": {
        "id": "03uF-V5DwfBJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "SA8Wp9-xwiZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_verbosity(input_list, target_list):\n",
        "  \"\"\"\n",
        "  input: list of source & target sequences\n",
        "  output: processed source sequence based on the calculated length ratios \n",
        "  \"\"\"\n",
        "  processed_input = []\n",
        "  for input, target in zip(input_list, target_list):\n",
        "    ts_ratio = len(target)/len(input)\n",
        "    if ts_ratio < 0.95:\n",
        "      prefix = \"short\"\n",
        "    elif ts_ratio >= 0.95 and ts_ratio <= 1.10:\n",
        "      prefix = \"normal\"\n",
        "    else:\n",
        "      prefix = \"long\"\n",
        "    input = prefix + \" \" + input\n",
        "    processed_input.append(input)\n",
        "  return processed_input"
      ],
      "metadata": {
        "id": "_HRpnd8vxUGJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "source_lang = \"en\"\n",
        "target_lang = \"de\"\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    inputs = add_verbosity(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "UGTxkmTjxXzy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_train_datasets = raw_datasets['train'].map(preprocess_function, batched=True)\n",
        "tokenized_validation_datasets = raw_datasets['validation'].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "6_dRmTpvxv8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "WkukCDlFx_T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16 # change batch-size according to GPU availability \n",
        "model_name = train_model_checkpoints.split(\"/\")[-1]\n",
        "\n",
        "# define training model arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "# initialize data-collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "DSGM8H8wypVk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "nvp7qi8Eyvi_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the trainer module\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_validation_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "MmYgfosoywyZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "05-KNvDrzGOk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}