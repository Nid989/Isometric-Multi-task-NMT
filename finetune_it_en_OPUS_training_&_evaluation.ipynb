{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune it-en OPUS training & evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Isometric-Multi-task-NMT/blob/main/finetune_it_en_OPUS_training_%26_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPpIQCUTu9pf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# un-comment below, while working on colab.\n",
        "!pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece] wandb boto3 --quiet \n",
        "!pip install -U nltk # upgrade current version of NLTK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "import datasets\n",
        "import boto3\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm \n",
        "import wandb\n",
        "import logging\n",
        "import pandas as pd "
      ],
      "metadata": {
        "id": "1sNQTEXAwBB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()"
      ],
      "metadata": {
        "id": "66w7-yvkQg7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"it\" "
      ],
      "metadata": {
        "id": "CxFg2v2I-j2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for logging loss to wandb.ai\n",
        "access_key = \"c7deb1bb77ce9433eb246d460385f363659145a8\" # enter wandb secret_accces_key\n",
        "wandb.login(key=access_key)   "
      ],
      "metadata": {
        "id": "3f0qpSQnwM1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data processing\n",
        "raw_datasets = load_dataset(f\"enimai/MuST-C-{target_lang}\")\n",
        "sacrebleu = load_metric(\"sacrebleu\")\n",
        "meteor = load_metric('meteor')"
      ],
      "metadata": {
        "id": "SS5bSKnSwOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_datasets)"
      ],
      "metadata": {
        "id": "luBld-SZwSPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model checkpoints\n",
        "train_model_checkpoints = f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\""
      ],
      "metadata": {
        "id": "03uF-V5DwfBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "SA8Wp9-xwiZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_verbosity(input_list, target_list):\n",
        "  \"\"\"\n",
        "  input: list of source & target sequences\n",
        "  output: processed source sequence based on the calculated length ratios \n",
        "  \"\"\"\n",
        "  processed_input = []\n",
        "  for input, target in zip(input_list, target_list):\n",
        "    ts_ratio = len(target)/len(input)\n",
        "    if ts_ratio < 0.90:\n",
        "      prefix = \"short\"\n",
        "    elif ts_ratio >= 0.90 and ts_ratio <= 1.10:\n",
        "      prefix = \"normal\"\n",
        "    else:\n",
        "      prefix = \"long\"\n",
        "    input = prefix + \" \" + input\n",
        "    processed_input.append(input)\n",
        "  return processed_input"
      ],
      "metadata": {
        "id": "_HRpnd8vxUGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    inputs = add_verbosity(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "UGTxkmTjxXzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_train_datasets = raw_datasets[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_validation_datasets = raw_datasets[\"validation\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "oB05-avda30v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "xYHxQZQ_a5NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2 # change batch-size according to GPU availability \n",
        "model_name = train_model_checkpoints.split(\"/\")[-1]\n",
        "epoch = 1\n",
        "strategy = \"steps\"\n",
        "steps_ = 2000\n",
        "save_steps_ = 2000\n",
        "\n",
        "# define training model arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    learning_rate=5e-5, \n",
        "    logging_strategy=strategy,\n",
        "    logging_steps=steps_,\n",
        "    # learning_rate=0.0003,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "    # warmup_ratio=0.06,\n",
        "    optim=\"adafactor\",\n",
        "    save_strategy=strategy,\n",
        "    save_steps=save_steps_,\n",
        "    evaluation_strategy=strategy,\n",
        "    eval_steps=steps_,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epoch,\n",
        "    report_to=\"wandb\",\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "  # initialize data-collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "X6fvaFlta8ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    sacrebleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    # meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        \"bleu\": sacrebleu_result[\"score\"],\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "tqcZ2c-Ja73m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the trainer module\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_validation_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "EsysLXmbzEE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_VUpqToZyeUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compress model checkpoint directory\n",
        "model_checkpoint_directory = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\"\n",
        "print(model_checkpoint_directory)\n",
        "shutil.make_archive(model_checkpoint_directory, \"zip\", model_checkpoint_directory)"
      ],
      "metadata": {
        "id": "uQa5__lDyfya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_checkpoint_directory}\"\n",
        "filename = f\"{model_checkpoint_directory}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "Tvnix9leaTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete checkpoint directory\n",
        "path_to_directory = os.path.join(current_directory, model_checkpoint_directory)\n",
        "shutil.rmtree(path_to_directory)"
      ],
      "metadata": {
        "id": "xDVfaecBdtDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete zip file\n",
        "path_to_zip_file = os.path.join(current_directory, filename)\n",
        "os.remove(path_to_zip_file)"
      ],
      "metadata": {
        "id": "XHZDzdqRd8uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "-eSTgynEPmpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"opus-mt-en-it\"\n",
        "epoch = 1\n",
        "model_checkpoint_directory = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\""
      ],
      "metadata": {
        "id": "jcmqC5XrPnw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_checkpoint_directory}\"\n",
        "filename = f\"{model_checkpoint_directory}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "X-35-xWXRCvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "path_to_zipfile = os.path.join(current_directory, f\"{model_checkpoint_directory}.zip\")\n",
        "path_to_output_directory = os.path.join(current_directory, f\"{model_checkpoint_directory}/\")\n",
        "shutil.unpack_archive(path_to_zipfile, path_to_output_directory)"
      ],
      "metadata": {
        "id": "edDuJY9gRUKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model checkpoints\n",
        "evaluation_model_checkpoint = os.path.join(path_to_output_directory, os.listdir(path_to_output_directory)[0])"
      ],
      "metadata": {
        "id": "GjgzsaDJShNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "jqAEVGUkTQDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_verbosity_eval(input_list, target_list):\n",
        "  \"\"\"\n",
        "  input: list of source & target sequences\n",
        "  output: processed source sequence based on the calculated length ratios \n",
        "  \"\"\"\n",
        "  processed_input = []\n",
        "  for input, target in zip(input_list, target_list):\n",
        "    ts_ratio = len(target)/len(input)\n",
        "    prefix = \"normal\"\n",
        "    input = prefix + \" \" + input\n",
        "    processed_input.append(input)\n",
        "  return processed_input"
      ],
      "metadata": {
        "id": "ZNsqY7tCTRoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "def preprocess_test_function(examples):\n",
        "    inputs = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    inputs = add_verbosity_eval(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "ZtgZldEiUx6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_test_dataset = raw_datasets[\"test\"].map(preprocess_test_function, batched=True)"
      ],
      "metadata": {
        "id": "yHDJbRzdU1At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "ETuc64QoU30m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, num_workers=0)"
      ],
      "metadata": {
        "id": "gKRu0FNBVIwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate model prediction\n",
        "predictions = []\n",
        "for batch in tqdm(test_dataloader, total=tokenized_test_dataset.shape[0]):\n",
        "  translated = model.generate(**tokenizer(batch['en'], return_tensors=\"pt\", padding=True))\n",
        "  predictions.extend([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
      ],
      "metadata": {
        "id": "dTNXmx71VLeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_source, test_target = [], []\n",
        "for instance in tqdm(raw_datasets[\"test\"], total=raw_datasets[\"test\"].shape[0]):\n",
        "  test_source.append(instance[source_lang])\n",
        "  test_target.append(instance[target_lang])\n",
        "\n",
        "# generate output prediction dataframe\n",
        "df = pd.DataFrame({\n",
        "    source_lang: test_source, \n",
        "    target_lang: test_target,\n",
        "    'mt_pred': predictions\n",
        "})"
      ],
      "metadata": {
        "id": "ZGtUgsZAVN5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_prediction_file = os.path.join(current_directory, f\"{model_name}-finetuned-{source_lang}-to-{target_lang}-predictions.csv\")\n",
        "df.to_csv(path_to_prediction_file, index=False)"
      ],
      "metadata": {
        "id": "QYJ46CUKVQJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload to s3\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}-predictions\"\n",
        "filename = path_to_prediction_file\n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "EGU6-emXVWbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete checkpoint directory\n",
        "shutil.rmtree(path_to_output_directory)"
      ],
      "metadata": {
        "id": "A6eXvPBEVWYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete zip files\n",
        "os.remove(f\"{model_checkpoint_directory}.zip\") # model checkpoints\n",
        "os.remove(path_to_prediction_file) # csv file"
      ],
      "metadata": {
        "id": "TUpxFMv7VaJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}