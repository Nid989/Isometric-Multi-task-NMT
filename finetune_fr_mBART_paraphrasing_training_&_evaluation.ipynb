{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune fr mBART paraphrasing training & evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Isometric-Multi-task-NMT/blob/main/finetune_fr_mBART_paraphrasing_training_%26_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "opusparcus: short sentences; useful to train model on common/ daily used language terms and slang \n",
        "paws-x: normal and long sentences; acquaint model with in-depth paraphrase reordering and chnages in wordings\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "94XIcDlGMaBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAJpcShOTgVL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers sentencepiece boto3 sacrebleu wandb datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import os\n",
        "import boto3\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from tqdm.notebook import tqdm\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "tqdm.pandas()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "T793qcTjVix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current working dire  \n",
        "current_directory = os.getcwd()"
      ],
      "metadata": {
        "id": "oWurTXWYilGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for logging loss to wandb.ai\n",
        "access_key = \"c7deb1bb77ce9433eb246d460385f363659145a8\" # enter wandb secret_accces_key\n",
        "wandb.login(key=access_key)   "
      ],
      "metadata": {
        "id": "Z6scithwg2c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opusparcus_data = load_dataset(\"GEM/opusparcus\", lang=\"fr\", quality=95)"
      ],
      "metadata": {
        "id": "I5JzgikSTj_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_opusparcus_data(data, data_type, train=False):\n",
        "  path_to_data_type = os.path.join(current_directory, f\"opusparcus_{data_type}.csv\")\n",
        "  data.to_csv(path_to_data_type, index=False)\n",
        "  df = pd.read_csv(path_to_data_type)\n",
        "  df.drop([\"lang\", \"gem_id\", \"references\", \"annot_score\"], axis=1, inplace=True)\n",
        "  df.rename(columns={\n",
        "      'input': 'input_text',\n",
        "      'target': 'target_text'\n",
        "  }, inplace=True)\n",
        "  if train:\n",
        "    df = df.sample(200000).reset_index(drop=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "UN13xuUDZlL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save train, test and validation data locally for further processing\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  if data_type==\"train\":\n",
        "    process_opusparcus_data(opusparcus_data[data_type], data_type=data_type, train=True)\n",
        "  else:\n",
        "    process_opusparcus_data(opusparcus_data[data_type], data_type=data_type)"
      ],
      "metadata": {
        "id": "o2vZi21SWGYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pawsx_data = load_dataset(\"paws-x\", \"fr\")"
      ],
      "metadata": {
        "id": "2MYveXBAaIE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pawsx_data(data, data_type):\n",
        "  path_to_data_type = os.path.join(current_directory, f\"pawsx_{data_type}.csv\")\n",
        "  data.to_csv(path_to_data_type, index=False)\n",
        "  df = pd.read_csv(path_to_data_type)\n",
        "  df.drop([\"id\", \"label\"], axis=1, inplace=True)\n",
        "  df.rename(columns={\n",
        "      'sentence1': 'input_text',\n",
        "      'sentence2': 'target_text'\n",
        "  }, inplace=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "mnij1luja6HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save train, test and validation data locally for further processing\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  process_pawsx_data(pawsx_data[data_type], data_type=data_type)"
      ],
      "metadata": {
        "id": "hOG6pnYUb8Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_opusparcus_and_pawsx_data(data_type):\n",
        "  path_to_dataset_a_data_type = os.path.join(current_directory, f\"{dataset_a}_{data_type}.csv\")\n",
        "  path_to_dataset_b_data_type = os.path.join(current_directory, f\"{dataset_b}_{data_type}.csv\")\n",
        "  df_a = pd.read_csv(path_to_dataset_a_data_type)\n",
        "  df_b = pd.read_csv(path_to_dataset_b_data_type)\n",
        "  df = pd.concat([df_a, df_b], axis=0)\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  os.remove(path_to_dataset_a_data_type)\n",
        "  os.remove(path_to_dataset_b_data_type)\n",
        "  path_to_data_type = os.path.join(current_directory, f\"{data_type}.csv\")\n",
        "  df.dropna(inplace=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "RjS674gvdPvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge opusparcus and pawsx dataset\n",
        "dataset_a = \"opusparcus\"\n",
        "dataset_b = \"pawsx\"\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  process_data_opusparcus_and_pawsx_data(data_type)  "
      ],
      "metadata": {
        "id": "kRq6NsZocDxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_checkpoints = \"facebook/mbart-large-50\""
      ],
      "metadata": {
        "id": "N6asO19hkYmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(train_model_checkpoints, src_lang=\"fr_XX\", tgt_lang=\"fr_XX\")"
      ],
      "metadata": {
        "id": "3Y4nxVACkcUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_prompt(source_list, target_list):\n",
        "  processed_input = []\n",
        "  for input, target in zip(source_list, target_list):\n",
        "    ts_ratio = len(target)/len(input)\n",
        "    if ts_ratio < 0.95:\n",
        "      prefix = \"paraphrase short\"\n",
        "    elif ts_ratio >= 0.95 and ts_ratio <= 1.10:\n",
        "      prefix = \"paraphrase normal\"\n",
        "    else:\n",
        "      prefix = \"paraphrase long\"\n",
        "    input = prefix + \" \" + input\n",
        "    processed_input.append(input)\n",
        "  return processed_input"
      ],
      "metadata": {
        "id": "2X22_OcYeHA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input_text\"]\n",
        "    targets = examples[\"target_text\"]\n",
        "    inputs = add_prompt(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "-yIynk8nkQTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_train = pd.read_csv(\"train.csv\")\n",
        "# sample_train = sample_train.sample(5000).reset_index(drop=True)\n",
        "# sample_train.to_csv(\"sample_train.csv\", index=False)\n",
        "# sample_validation = pd.read_csv(\"validation.csv\")\n",
        "# sample_validation = sample_validation.sample(1000).reset_index(drop=True)\n",
        "# sample_validation.to_csv(\"sample_validation.csv\", index=False)"
      ],
      "metadata": {
        "id": "sxoYD4oYjVHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_train_data = os.path.join(current_directory, \"train.csv\")\n",
        "# path_to_validation_data = os.path.join(current_directory, \"validation.csv\")\n",
        "path_to_train_data = os.path.join(current_directory, \"sample_train.csv\")\n",
        "path_to_validation_data = os.path.join(current_directory, \"sample_validation.csv\")\n",
        "raw_train_dataset = load_dataset('csv', data_files={\"train\": path_to_train_data})\n",
        "raw_validation_dataset = load_dataset('csv', data_files={\"validation\": path_to_validation_data})"
      ],
      "metadata": {
        "id": "uQM_O1wLgBTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_train_datasets = raw_train_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_validation_datasets = raw_validation_dataset[\"validation\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "m7Pdj6mZpgXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = MBartForConditionalGeneration.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "8allrAVnghNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 # change batch-size according to GPU availability \n",
        "model_name = train_model_checkpoints.split(\"/\")[-1]\n",
        "epoch = 2\n",
        "lang = \"fr\"\n",
        "strategy = \"steps\"\n",
        "steps_ = 500\n",
        "save_steps_ = 500\n",
        "\n",
        "# define training model arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"fewshot-learning-{model_name}-paraphrase-finetuned-for-{lang}\",\n",
        "    learning_rate=5e-5, \n",
        "    logging_strategy=strategy,\n",
        "    logging_steps=steps_,\n",
        "    # learning_rate=0.0003,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "    # warmup_ratio=0.06,\n",
        "    optim=\"adafactor\",\n",
        "    save_strategy=strategy,\n",
        "    save_steps=save_steps_,\n",
        "    evaluation_strategy=strategy,\n",
        "    eval_steps=steps_,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epoch,\n",
        "    report_to=\"wandb\",\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "# initialize data-collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "U6cLMMcWhNJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sacrebleu = load_metric(\"sacrebleu\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    sacrebleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    # meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        \"bleu\": sacrebleu_result[\"score\"],\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "jPOvjOI5jYJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the trainer module\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_validation_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "EBWGqtxirB7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Usr-bx_vgZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compress model checkpoint directory\n",
        "model_checkpoints = f\"fewshot-learning-{model_name}-paraphrase-finetuned-for-{lang}\"\n",
        "model_checkpoint_directory = os.path.join(current_directory, model_checkpoints)\n",
        "print(model_checkpoint_directory)\n",
        "shutil.make_archive(model_checkpoint_directory, \"zip\", model_checkpoint_directory.split('/')[-1])"
      ],
      "metadata": {
        "id": "8v1g6a59sRRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_checkpoints}\"\n",
        "filename = f\"{model_checkpoints}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "Tvnix9leaTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete checkpoint directory\n",
        "current_directory = os.getcwd()\n",
        "path_to_directory = os.path.join(current_directory, model_checkpoints)\n",
        "shutil.rmtree(path_to_directory)"
      ],
      "metadata": {
        "id": "xDVfaecBdtDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete zip file\n",
        "current_directory = os.getcwd()\n",
        "path_to_zip_file = os.path.join(current_directory, filename)\n",
        "os.remove(path_to_zip_file)"
      ],
      "metadata": {
        "id": "XHZDzdqRd8uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "fvSdkh73DZlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = \"opus-mt-en-fr-predictions\"\n",
        "filename = \"/content/opus-mt-en-fr-predictions.csv\"\n",
        "print(key)\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "QoqB4VqDDpvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_predfile = os.path.join(current_directory, \"opus-mt-en-fr-predictions.csv\")\n",
        "pred_df = pd.read_csv(path_to_predfile)"
      ],
      "metadata": {
        "id": "3ZWi6UaTEhzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_verbosity(input_text, target_text):\n",
        "  ts_ratio = len(target_text)/len(input_text)\n",
        "  if not (ts_ratio >= 0.90 and ts_ratio <= 1.10):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def append_paraphrase_prompt(input_text, target_text):\n",
        "  ts_ratio = len(target_text)/len(input_text)\n",
        "  prefix = None\n",
        "  if ts_ratio < 0.90:\n",
        "    prefix = \"paraphrase long\"\n",
        "  elif ts_ratio > 1.10:\n",
        "    prefix = \"paraphrase short\"\n",
        "  target_text = prefix + \" \" + target_text\n",
        "  return target_text"
      ],
      "metadata": {
        "id": "zIF4P9bVFExw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if mt_prediction -> input length ratio is normal\n",
        "pred_df[\"is_normal\"] = pred_df.progress_apply(\n",
        "    lambda row: check_for_verbosity(row['en'], row['mt_pred']),\n",
        "    axis=1\n",
        ")\n",
        "not_normal_seq_index = pred_df.index[pred_df['is_normal'] == True].to_list()"
      ],
      "metadata": {
        "id": "P4fAlQ-5FGe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"en\", \"fr\", \"mt_pred\"]\n",
        "pred_normal_df = pred_df[~pred_df.index.isin(not_normal_seq_index)][columns]\n",
        "pred_not_normal_df = pred_df[pred_df.index.isin(not_normal_seq_index)][columns]\n",
        "\n",
        "# apply paraphrase prompt \n",
        "pred_not_normal_df[\"mt_pred\"] = pred_not_normal_df.progress_apply(\n",
        "    lambda row: append_paraphrase_prompt(row['en'], row['mt_pred']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "path_to_not_normal_file = os.path.join(current_directory, \"test_not_normal.csv\")\n",
        "path_to_normal_file = os.path.join(current_directory, \"test_normal.csv\")\n",
        "pred_not_normal_df.to_csv(path_to_not_normal_file, index=False)\n",
        "pred_normal_df.to_csv(path_to_normal_file, index=False)"
      ],
      "metadata": {
        "id": "JksRFg7pFIiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 2\n",
        "model_name = \"fewshot-learning-mbart-large-50-paraphrase-finetuned-for-fr\""
      ],
      "metadata": {
        "id": "lGKaHa0CFLtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_name}\"\n",
        "filename = f\"{model_name}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "GPkY-EtgFQGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "path_to_zipfile = os.path.join(current_directory, f\"{model_name}.zip\")\n",
        "path_to_output_directory = os.path.join(current_directory, f\"{model_name}/\")\n",
        "shutil.unpack_archive(path_to_zipfile, path_to_output_directory)"
      ],
      "metadata": {
        "id": "14H1qli3FZD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model checkpoints\n",
        "evaluation_model_checkpoint = os.path.join(path_to_output_directory, os.listdir(path_to_output_directory)[0])"
      ],
      "metadata": {
        "id": "ZiOmbFV_Fmao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "JFrGJ1IxFoJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = MBartForConditionalGeneration.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "giG0XFdLF01A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_raw_test_dataset = load_dataset('csv', data_files={\"test\": path_to_not_normal_file})"
      ],
      "metadata": {
        "id": "NnTO-UusF7Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(processed_raw_test_dataset[\"test\"], batch_size=1, num_workers=0)"
      ],
      "metadata": {
        "id": "wxAjeF3_F_vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate model prediction\n",
        "predictions = []\n",
        "for batch in tqdm(test_dataloader):\n",
        "  translated = model.generate(**tokenizer(batch['mt_pred'], return_tensors=\"pt\", padding=True))\n",
        "  predictions.extend([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
      ],
      "metadata": {
        "id": "dF-cB6L0GF7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_not_normal_df[\"mt_pred\"] = predictions\n",
        "processed_pred_df = pd.concat([pred_normal_df, pred_not_normal_df]).sort_index()\n",
        "predfile_name = f\"{model_name}-predictions.csv\"\n",
        "path_to_processed_predfile = os.path.join(current_directory, predfile_name)\n",
        "processed_pred_df.to_csv(path_to_processed_predfile, index=False)"
      ],
      "metadata": {
        "id": "V8DzxJpbGDBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = predfile_name.split('.')[0] # fewshot-learning-mbart-large-50-paraphrase-finetuned-for-fr-predictions\n",
        "filename = path_to_processed_predfile \n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "26aflWb_GMQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(path_to_output_directory)"
      ],
      "metadata": {
        "id": "n-Zk3BOGGOCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(path_to_zipfile)\n",
        "os.remove(path_to_predfile)\n",
        "os.remove(path_to_not_normal_file)\n",
        "os.remove(path_to_normal_file)\n",
        "os.remove(path_to_processed_predfile)"
      ],
      "metadata": {
        "id": "OC01r-GEGPaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}