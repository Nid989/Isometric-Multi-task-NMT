{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune de mBART paraphrasing training & evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4KSxGIzI8WeP",
        "swGmGnpi8Yyf",
        "mtyHPk_K9PiN"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Isometric-Multi-task-NMT/blob/main/finetune_de_mBART_paraphrasing_training_%26_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "opusparcus: short sentences; useful to train model on common/ daily used language terms and slang \n",
        "paws-x: normal and long sentences; acquaint model with in-depth paraphrase reordering and chnages in wordings\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "94XIcDlGMaBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAJpcShOTgVL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers sentencepiece boto3 sacrebleu wandb datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk -U --quiet"
      ],
      "metadata": {
        "id": "saL5fmp_Xz3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import os\n",
        "import boto3\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer, MarianMTModel, MarianTokenizer\n",
        "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
        "from tqdm.notebook import tqdm\n",
        "import logging\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "tqdm.pandas()\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "T793qcTjVix7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# current working directory\n",
        "current_directory = os.getcwd()"
      ],
      "metadata": {
        "id": "SJIYmJ51YnaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for logging loss to wandb.ai\n",
        "access_key = \"c7deb1bb77ce9433eb246d460385f363659145a8\" # enter wandb secret_accces_key\n",
        "wandb.login(key=access_key)"
      ],
      "metadata": {
        "id": "Z6scithwg2c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opusparcus_data = load_dataset(\"GEM/opusparcus\", lang=\"de\", quality=95)"
      ],
      "metadata": {
        "id": "I5JzgikSTj_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_opusparcus_data(data, data_type):\n",
        "  path_to_data_type = os.path.join(current_directory, f\"opusparcus_{data_type}.csv\")\n",
        "  data.to_csv(path_to_data_type, index=False)\n",
        "  df = pd.read_csv(path_to_data_type)\n",
        "  df.drop([\"lang\", \"gem_id\", \"references\", \"annot_score\"], axis=1, inplace=True)\n",
        "  df.rename(columns={\n",
        "      'input': 'input_text',\n",
        "      'target': 'target_text'\n",
        "  }, inplace=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "UN13xuUDZlL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save train, test and validation data locally for further processing\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  process_opusparcus_data(opusparcus_data[data_type], data_type=data_type)"
      ],
      "metadata": {
        "id": "o2vZi21SWGYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pawsx_data = load_dataset(\"paws-x\", \"de\")"
      ],
      "metadata": {
        "id": "2MYveXBAaIE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pawsx_data(data, data_type):\n",
        "  path_to_data_type = os.path.join(current_directory, f\"pawsx_{data_type}.csv\")\n",
        "  data.to_csv(path_to_data_type, index=False)\n",
        "  df = pd.read_csv(path_to_data_type)\n",
        "  df.drop([\"id\", \"label\"], axis=1, inplace=True)\n",
        "  df.rename(columns={\n",
        "      'sentence1': 'input_text',\n",
        "      'sentence2': 'target_text'\n",
        "  }, inplace=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "mnij1luja6HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save train, test and validation data locally for further processing\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  process_pawsx_data(pawsx_data[data_type], data_type=data_type)"
      ],
      "metadata": {
        "id": "hOG6pnYUb8Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_opusparcus_and_pawsx_data(data_type):\n",
        "  path_to_dataset_a_data_type = os.path.join(current_directory, f\"{dataset_a}_{data_type}.csv\")\n",
        "  path_to_dataset_b_data_type = os.path.join(current_directory, f\"{dataset_b}_{data_type}.csv\")\n",
        "  df_a = pd.read_csv(path_to_dataset_a_data_type)\n",
        "  df_b = pd.read_csv(path_to_dataset_b_data_type)\n",
        "  df = pd.concat([df_a, df_b], axis=0)\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  os.remove(path_to_dataset_a_data_type)\n",
        "  os.remove(path_to_dataset_b_data_type)\n",
        "  path_to_data_type = os.path.join(current_directory, f\"{data_type}.csv\")\n",
        "  df.dropna(inplace=True)\n",
        "  df.to_csv(path_to_data_type, index=False)"
      ],
      "metadata": {
        "id": "RjS674gvdPvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge opusparcus and pawsx dataset\n",
        "dataset_a = \"opusparcus\"\n",
        "dataset_b = \"pawsx\"\n",
        "data_types = [\"train\", \"test\", \"validation\"]\n",
        "for data_type in tqdm(data_types, total=len(data_types)):\n",
        "  process_data_opusparcus_and_pawsx_data(data_type)  "
      ],
      "metadata": {
        "id": "kRq6NsZocDxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_checkpoints = \"facebook/mbart-large-50\""
      ],
      "metadata": {
        "id": "N6asO19hkYmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(train_model_checkpoints, src_lang=\"de_DE\", tgt_lang=\"de_DE\")"
      ],
      "metadata": {
        "id": "3Y4nxVACkcUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_prompt(source_list, target_list):\n",
        "  processed_input = []\n",
        "  for input, target in zip(source_list, target_list):\n",
        "    ts_ratio = len(target)/len(input)\n",
        "    if ts_ratio < 0.95:\n",
        "      prefix = \"paraphrase short\"\n",
        "    elif ts_ratio >= 0.95 and ts_ratio <= 1.10:\n",
        "      prefix = \"paraphrase normal\"\n",
        "    else:\n",
        "      prefix = \"paraphrase long\"\n",
        "    input = prefix + \" \" + input\n",
        "    processed_input.append(input)\n",
        "  return processed_input"
      ],
      "metadata": {
        "id": "2X22_OcYeHA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess MUST-C dataset\n",
        "max_input_length = 128 \n",
        "max_target_length = 128\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"input_text\"]\n",
        "    targets = examples[\"target_text\"]\n",
        "    inputs = add_prompt(inputs, targets) # append appropriate prompts \n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    # setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "-yIynk8nkQTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_train = pd.read_csv(\"train.csv\")\n",
        "# sample_train = sample_train.sample(5000).reset_index(drop=True)\n",
        "# sample_train.to_csv(\"sample_train.csv\", index=False)\n",
        "# sample_validation = pd.read_csv(\"validation.csv\")\n",
        "# sample_validation = sample_validation.sample(1000).reset_index(drop=True)\n",
        "# sample_validation.to_csv(\"sample_validation.csv\", index=False)"
      ],
      "metadata": {
        "id": "QM89Aut1rp33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path_to_train_data = os.path.join(current_directory, \"train.csv\")\n",
        "path_to_train_data = os.path.join(current_directory, \"sample_train.csv\")\n",
        "# path_to_validation_data = os.path.join(current_directory, \"validation.csv\")\n",
        "path_to_validation_data = os.path.join(current_directory, \"sample_validation.csv\")\n",
        "raw_train_dataset = load_dataset('csv', data_files={\"train\": path_to_train_data})\n",
        "raw_validation_dataset = load_dataset('csv', data_files={\"validation\": path_to_validation_data})"
      ],
      "metadata": {
        "id": "uQM_O1wLgBTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize raw data\n",
        "tokenized_train_datasets = raw_train_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_validation_datasets = raw_validation_dataset[\"validation\"].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "m7Pdj6mZpgXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = MBartForConditionalGeneration.from_pretrained(train_model_checkpoints)"
      ],
      "metadata": {
        "id": "8allrAVnghNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2 # change batch-size according to GPU availability \n",
        "model_name = train_model_checkpoints.split(\"/\")[-1]\n",
        "epoch = 2\n",
        "lang = \"de\"\n",
        "strategy = \"steps\"\n",
        "steps_ = 500\n",
        "save_steps_ = 500\n",
        "\n",
        "# define training model arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"fewshot-learning-{model_name}-paraphrase-finetuned-for-{lang}\",\n",
        "    learning_rate=5e-5, \n",
        "    logging_strategy=strategy,\n",
        "    logging_steps=steps_,\n",
        "    # learning_rate=0.0003,\n",
        "    # lr_scheduler_type=\"linear\",\n",
        "    # warmup_ratio=0.06,\n",
        "    optim=\"adafactor\",\n",
        "    save_strategy=strategy,\n",
        "    save_steps=save_steps_,\n",
        "    evaluation_strategy=strategy,\n",
        "    eval_steps=steps_,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=epoch,\n",
        "    report_to=\"wandb\",\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=True    \n",
        ")\n",
        "\n",
        "# initialize data-collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "U6cLMMcWhNJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sacrebleu = load_metric(\"sacrebleu\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "    sacrebleu_result = sacrebleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\n",
        "        \"bleu\": sacrebleu_result[\"score\"],\n",
        "        \"meteor\": meteor_result['meteor']\n",
        "    }\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "jPOvjOI5jYJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the trainer module\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_validation_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "EBWGqtxirB7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Usr-bx_vgZzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # compress model checkpoint directory\n",
        "model_name = f\"fewshot-learning-{model_name}-paraphrase-finetuned-for-{lang}\"\n",
        "model_checkpoint_directory = os.path.join(current_directory, f\"fewshot-learning-{model_name}-paraphrase-finetuned-for-{lang}\")\n",
        "print(model_checkpoint_directory)\n",
        "shutil.make_archive(model_checkpoint_directory, \"zip\", model_checkpoint_directory.split('/')[-1])"
      ],
      "metadata": {
        "id": "yYheEyGxyKfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_name}\"\n",
        "filename = f\"{model_checkpoint_directory}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "Tvnix9leaTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete checkpoint directory\n",
        "current_directory = os.getcwd()\n",
        "path_to_directory = os.path.join(current_directory, model_checkpoint_directory)\n",
        "shutil.rmtree(path_to_directory)"
      ],
      "metadata": {
        "id": "xDVfaecBdtDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete zip file\n",
        "current_directory = os.getcwd()\n",
        "path_to_zip_file = os.path.join(current_directory, filename)\n",
        "os.remove(path_to_zip_file)"
      ],
      "metadata": {
        "id": "XHZDzdqRd8uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "Ucan33tGt35J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download MT predictions\n",
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = \"opus-mt-en-de-predictions\"\n",
        "filename = \"opus-mt-en-de-predictions.csv\"\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "UIsoDJkj73fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_predfile = os.path.join(current_directory, \"opus-mt-en-de-predictions.csv\")\n",
        "pred_df = pd.read_csv(path_to_predfile)"
      ],
      "metadata": {
        "id": "pkuIDpax8KI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_verbosity(input_text, target_text):\n",
        "  ts_ratio = len(target_text)/len(input_text)\n",
        "  if not (ts_ratio >= 0.90 and ts_ratio <= 1.10):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def append_paraphrase_prompt(input_text, target_text):\n",
        "  ts_ratio = len(target_text)/len(input_text)\n",
        "  prefix = None\n",
        "  if ts_ratio < 0.90:\n",
        "    prefix = \"paraphrase long\"\n",
        "  elif ts_ratio > 1.10:\n",
        "    prefix = \"paraphrase short\"\n",
        "  target_text = prefix + \" \" + target_text\n",
        "  return target_text"
      ],
      "metadata": {
        "id": "6jv5MZWb-P5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if mt_prediction -> input length ratio is normal\n",
        "pred_df[\"is_normal\"] = pred_df.progress_apply(\n",
        "    lambda row: check_for_verbosity(row['en'], row['mt_pred']),\n",
        "    axis=1\n",
        ")\n",
        "not_normal_seq_index = pred_df.index[pred_df['is_normal'] == True].to_list()"
      ],
      "metadata": {
        "id": "P8f2j57n-IiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"en\", \"de\", \"mt_pred\"]\n",
        "pred_normal_df = pred_df[~pred_df.index.isin(not_normal_seq_index)][columns]\n",
        "pred_not_normal_df = pred_df[pred_df.index.isin(not_normal_seq_index)][columns]\n",
        "\n",
        "# apply paraphrase prompt \n",
        "pred_not_normal_df[\"mt_pred\"] = pred_not_normal_df.progress_apply(\n",
        "    lambda row: append_paraphrase_prompt(row['en'], row['mt_pred']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "path_to_not_normal_file = os.path.join(current_directory, \"test_not_normal.csv\")\n",
        "path_to_normal_file = os.path.join(current_directory, \"test_normal.csv\")\n",
        "pred_not_normal_df.to_csv(path_to_not_normal_file, index=False)\n",
        "pred_normal_df.to_csv(path_to_normal_file, index=False)"
      ],
      "metadata": {
        "id": "ZrYKE1P1_5QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 1\n",
        "model_name = \"mbart-large-50-paraphrase-finetuned-for-de\""
      ],
      "metadata": {
        "id": "th2VnfQVHjqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_name}\"\n",
        "filename = f\"{model_name}.zip\"\n",
        "print(key)\n",
        "s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "Ow5bcYTmuCz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_directory = os.getcwd()\n",
        "path_to_zipfile = os.path.join(current_directory, f\"{model_name}.zip\")\n",
        "path_to_output_directory = os.path.join(current_directory, f\"{model_name}/\")\n",
        "shutil.unpack_archive(path_to_zipfile, path_to_output_directory)"
      ],
      "metadata": {
        "id": "Z-C2Q9CbuH2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model checkpoints\n",
        "evaluation_model_checkpoint = os.path.join(path_to_output_directory, os.listdir(path_to_output_directory)[0])"
      ],
      "metadata": {
        "id": "euM_q2_Kuu2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the MarianMT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "EWjCdvI94YlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training procedure\n",
        "model = MBartForConditionalGeneration.from_pretrained(evaluation_model_checkpoint)"
      ],
      "metadata": {
        "id": "hnynPzXb42Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_raw_test_dataset = load_dataset('csv', data_files={\"test\": path_to_not_normal_file})"
      ],
      "metadata": {
        "id": "_ASuXtvgQtCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(processed_raw_test_dataset[\"test\"], batch_size=1, num_workers=0)"
      ],
      "metadata": {
        "id": "QWgNN4p_5JAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate model prediction\n",
        "predictions = []\n",
        "for batch in tqdm(test_dataloader):\n",
        "  translated = model.generate(**tokenizer(batch['mt_pred'], return_tensors=\"pt\", padding=True))\n",
        "  predictions.extend([tokenizer.decode(t, skip_special_tokens=True) for t in translated])\n",
        "\n",
        "pred_not_normal_df[\"mt_pred\"] = predictions\n",
        "processed_pred_df = pd.concat([pred_normal_df, pred_not_normal_df]).sort_index()\n",
        "predfile_name = f\"{model_name}-predictions.csv\"\n",
        "path_to_processed_predfile = os.path.join(current_directory, predfile_name)\n",
        "processed_pred_df.to_csv(path_to_processed_predfile, index=False)"
      ],
      "metadata": {
        "id": "Gaw_Bnb7RfnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = predfile_name.split('.')[0]\n",
        "filename = path_to_processed_predfile\n",
        "print(key)\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "EVzCn_OREQSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(path_to_output_directory)"
      ],
      "metadata": {
        "id": "1LGaCJF6FzFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove(path_to_zipfile)\n",
        "os.remove(path_to_predfile)\n",
        "os.remove(path_to_not_normal_file)\n",
        "os.remove(path_to_normal_file)\n",
        "os.remove(path_to_processed_predfile)"
      ],
      "metadata": {
        "id": "fKfuyxjzEXWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### example 1\n",
        "----\n",
        "* en_text: \"Heinz Kohut saw the grandiose self as a fixation on a normal childhood stage, while other post-Freudians examined the role of the fixation in aggression and delinquency.\"\n",
        "* de_text: \"Heinz Kohut sah das grandiose Selbst als Fixierung auf ein normales Kindheitstadium, während andere Post-Freudianer die Rolle der Fixierung bei Aggression und Kriminalität untersuchten.\"\n",
        "* target to source ratio: 1.0946745562130178 \n",
        "----\n",
        "* short_text: \"Das grandiose Selbst als Fixierung auf ein normales Kindheitstadium, während andere Post-Freudianer die Rolle der Fixierung bei Aggression und Kriminalität untersuchten\"\n",
        "* normal_text: \"Heinz Kohut sah das grandiose Selbst als Fixierung auf ein normales Kindheitstadium, während andere Post-Freudianer die Rolle der Fixierung bei Aggression und Kriminalität untersuchten.\"\n",
        "* long_text: \"Heinz Kohut sah das grandiose Selbst als Fixierung auf ein normales Kindheitstadium, während andere Post-Freudianer die Rolle der Fixierung bei Aggression und Kriminalität untersuchten.\"\n",
        "* target to source ratio: \n",
        "  * short: 0.9940828402366864\n",
        "  * normal: 1.0946745562130178\n",
        "  * long: 1.0946745562130178\n",
        "\n",
        "##### example 2\n",
        "----\n",
        "* en_text: \"In April 1942 Britten returned to England and shortly thereafter asked Montagu Slater to be his librettist for Peter Grimes.\"\n",
        "* de_text: \"Im April 1942 kehrte Britten zurück nach England, und kurz danach bat er Montagu Slater, sein Librettist für \\\"Peter Grimes\\\" zu werden.\"\n",
        "* target to source ratio: 1.0806451612903225\n",
        "----\n",
        "\n",
        "* short_text: \"In England kehrte er zurück, und kurz danach bat er Montagu Slater, sein Librettist für \"Peter Grimes\" zu werden.\"\n",
        "* normal_text: \"In April 1942 kehrte Britten zurück, und kurz danach bat er Montagu Slater, sein Librettist für \"Peter Grimes\" zu werden.\"\n",
        "* long_text: \"In England kehrte er zurück, und kurz danach bat er Montagu Slater, sein Librettist für \"Peter Grimes\" zu werden.\"\n",
        "* target to source ratio: \n",
        "  * short: 0.9112903225806451, \n",
        "  * normal: 0.9758064516129032\n",
        "  * long: 0.9112903225806451\n",
        "\n",
        "##### example 3\n",
        "----\n",
        "* en_text: \"Tell that to his father, Zac MacGuire (Charlie Clausen), and Evie right away.\"\n",
        "* de_text: \"Sag das sofort seinem Vater, Zac MacGuire (Charlie Clausen), und Evie .\"\n",
        "* target to source ratio: 0.922077922077922\n",
        "----\n",
        "* short_text: \"Sag es seinem Vater, Zac MacGuire, und Evie\"\n",
        "* normal_text: \"Sag es seinem Vater, Zac Guire (Charlie Clausen), und Evie.\"\n",
        "* long_text: \"Sag es seinem Vater, Zac Guire (Charlie Clausen), und Evie.\"\n",
        "* target to source ratio: \n",
        "  * short: 0.9112903225806451, \n",
        "  * normal: 0.9758064516129032\n",
        "  * long: 0.9112903225806451"
      ],
      "metadata": {
        "id": "K2RqPQGWIsAe"
      }
    }
  ]
}