{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune de-en MT5 training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nid989/Isometric-Multi-task-NMT/blob/main/finetune_de_en_MT5_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxSnX7v7W2P4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install simpletransformers boto3 wandb datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import boto3\n",
        "import os\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "# from tqdm import tqdm \n",
        "import wandb\n",
        "import shutil\n",
        "import logging\n",
        "import pandas as pd\n",
        "from simpletransformers.t5 import T5Model, T5Args\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "set_seed(7)\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "ZTkqRNZqYtNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for logging loss to wandb.ai\n",
        "access_key = \"c7deb1bb77ce9433eb246d460385f363659145a8\" # enter wandb secret_accces_key\n",
        "wandb.login(key=access_key)"
      ],
      "metadata": {
        "id": "bwu2m25JY6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_types = [\"train\", \"validation\"]\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "for data_type in tqdm(data_types, total=2):\n",
        "  session = boto3.Session(\n",
        "      aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "      aws_secret_access_key='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',\n",
        "  )\n",
        "  s3 = session.resource('s3')\n",
        "  key = f\"merged_de_en_{data_type}\"\n",
        "  filename = f\"{data_type}.csv\"\n",
        "  path_to_file = os.path.join(current_directory, filename)\n",
        "  s3.meta.client.download_file(Bucket='tsd2022', Key=key, Filename=path_to_file)"
      ],
      "metadata": {
        "id": "sDxu0tr3Zv-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_train_data = os.path.join(current_directory, \"train.csv\")\n",
        "train_dataset = pd.read_csv(path_to_train_data)\n",
        "\n",
        "path_to_validation_data = os.path.join(current_directory, \"validation.csv\")\n",
        "validation_dataset = pd.read_csv(path_to_validation_data)"
      ],
      "metadata": {
        "id": "0Fvk8e0HfQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = train_dataset.shape[0]\n",
        "sample_size = train_size * 0.15\n",
        "train_dataset = train_dataset.sample(n=int(sample_size))"
      ],
      "metadata": {
        "id": "n8meHVheDMHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_verbosity(input, target):\n",
        "  ts_ratio = len(target)/len(input)\n",
        "  if ts_ratio < 0.95:\n",
        "    prefix = \"short\"\n",
        "  elif ts_ratio >= 0.95 and ts_ratio <= 1.10:\n",
        "    prefix = \"normal\"\n",
        "  else:\n",
        "    prefix = \"long\"\n",
        "  return prefix + \" \" + input"
      ],
      "metadata": {
        "id": "Fugwmn80FZtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['en'] = train_dataset.progress_apply(\n",
        "    lambda row: add_verbosity(row['en'], row['de']),\n",
        "    axis=1\n",
        ")\n",
        "validation_dataset['en'] = validation_dataset.progress_apply(\n",
        "    lambda row: add_verbosity(row['en'], row['de']),\n",
        "    axis=1\n",
        ")"
      ],
      "metadata": {
        "id": "lUEbqW4hFyrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.reset_index(drop=True, inplace=True)\n",
        "train_dataset.rename(columns={\n",
        "    'en': 'input_text',\n",
        "    'de': 'target_text'\n",
        "}, inplace=True)\n",
        "\n",
        "validation_dataset.rename(columns={\n",
        "    'en': 'input_text',\n",
        "    'de': 'target_text'\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "wULlwgu2Pix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"en\"\n",
        "target_lang = \"de\"\n",
        "epoch = 2\n",
        "batch_size = 2\n",
        "model_checkpoints = \"google/mt5-base\"\n",
        "model_name = model_checkpoints.split('/')[-1]"
      ],
      "metadata": {
        "id": "PxsPjr4aGvsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "train_dataset[\"prefix\"] = \"\"\n",
        "validation_dataset[\"prefix\"] = \"\"\n",
        "\n",
        "model_args = T5Args()\n",
        "# model_args.max_seq_length = 100\n",
        "model_args.train_batch_size = batch_size\n",
        "model_args.eval_batch_size = batch_size\n",
        "model_args.num_train_epochs = epoch\n",
        "model_args.scheduler = \"cosine_schedule_with_warmup\"\n",
        "model_args.evaluate_during_training = True\n",
        "model_args.evaluate_during_training_steps = 10000\n",
        "model_args.learning_rate = 0.0003\n",
        "model_args.optimizer = 'Adafactor'\n",
        "model_args.use_multiprocessing = False\n",
        "model_args.fp16 = False\n",
        "model_args.save_steps = -1\n",
        "model_args.save_eval_checkpoints = False\n",
        "model_args.no_cache = True\n",
        "model_args.reprocess_input_data = True\n",
        "model_args.overwrite_output_dir = True\n",
        "model_args.save_model_every_epoch = True\n",
        "model_args.preprocess_inputs = False\n",
        "model_args.use_early_stopping = True\n",
        "model_args.num_return_sequences = 1\n",
        "model_args.do_lower_case = True\n",
        "model_args.output_dir = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}/\"\n",
        "model_args.best_model_dir = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}/best-model\"\n",
        "model_args.wandb_project = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}/\"\n",
        "\n",
        "model = T5Model(\"mt5\", model_checkpoints, args=model_args)\n",
        "#model.model.load_state_dict(torch.load(\"../input/semifinalyoruba/outputs/best_model/pytorch_model.bin\"))\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_dataset, eval_data=validation_dataset)\n",
        "\n",
        "# Optional: Evaluate the model. We'll test it properly anyway.\n",
        "results = model.eval_model(validation_dataset, verbose=True)"
      ],
      "metadata": {
        "id": "3SbKmLsFGHnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compress model checkpoint directory\n",
        "model_checkpoint_directory = f\"{model_name}-finetuned-{source_lang}-to-{target_lang}/\"\n",
        "print(model_checkpoint_directory)\n",
        "shutil.make_archive(model_checkpoint_directory, \"zip\", model_checkpoint_directory)"
      ],
      "metadata": {
        "id": "JhCuOId9Izwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = boto3.Session(\n",
        "    aws_access_key_id='AKIA4QB2WTN5YQGLD77G',\n",
        "    aws_secret_access_key='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',\n",
        ")\n",
        "s3 = session.resource('s3')\n",
        "key = f\"{epoch}_{model_checkpoint_directory}\"\n",
        "filename = f\"{model_checkpoint_directory}.zip\"\n",
        "s3.meta.client.upload_file(Bucket='tsd2022', Key=key, Filename=filename)"
      ],
      "metadata": {
        "id": "k6DDsz4XJpWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete checkpoint directory\n",
        "current_directory = os.getcwd()\n",
        "path_to_directory = os.path.join(current_directory, model_checkpoint_directory)\n",
        "shutil.rmtree(path_to_directory)"
      ],
      "metadata": {
        "id": "no44X679J0zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete zip file\n",
        "current_directory = os.getcwd()\n",
        "path_to_zip_file = os.path.join(current_directory, filename)\n",
        "os.remove(path_to_zip_file)"
      ],
      "metadata": {
        "id": "05RS7fKwJ2Pe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}